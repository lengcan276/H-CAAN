nohup: 忽略输入
Using device: cuda:0
Loading data...
Pre-processed data found: results/processed/processed/Lipophilicity_MFBERT_dataTrain.pt, loading ...
Pre-processed data found: results/processed/processed/Lipophilicity_MFBERT_dataTest.pt, loading ...
Initializing model...
Starting training...

Epoch 1/50
Original input_ids shape: torch.Size([8192])
Original attention_mask shape: torch.Size([8192])
Batch size from y: 16
Target shape: torch.Size([16, 1])
Target values: [[2.2 ]
 [0.42]
 [0.93]
 [0.61]
 [0.68]]
Processed input_ids shape: torch.Size([16, 512])
Processed attention_mask shape: torch.Size([16, 512])
Prediction 0 range: 2.0129 to 2.3609
Prediction 1 range: -0.0390 to 0.0446
Prediction 2 range: -44.5792 to -12.2225
Prediction 3 range: -0.7309 to 0.0720
Loss 0: 0.9239
Loss 1: 4.5489
Loss 2: 984.3796
Loss 3: 6.1855
Batch 0, Current loss: 249.0095
Batch 10, Current loss: 21.6486
Warning: NaN loss at batch 17
Batch 20, Current loss: 6.3118
Batch 30, Current loss: 5.1718
Batch 40, Current loss: 3.0536
Batch 50, Current loss: 2.7080
Batch 60, Current loss: 2.7187
Batch 70, Current loss: 1.2483
Warning: NaN loss at batch 71
Batch 80, Current loss: 2.7140
Batch 90, Current loss: 3.2256
Batch 100, Current loss: 2.9957
Warning: NaN loss at batch 110
Batch 120, Current loss: 2.1254
Batch 130, Current loss: 1.9577
Batch 140, Current loss: 1.9155
Warning: NaN loss at batch 142
Batch 150, Current loss: 1.5635
Warning: NaN loss at batch 157
Batch 160, Current loss: 2.2082
Warning: NaN loss at batch 168
Batch 170, Current loss: 2.6865
Warning: NaN loss at batch 174
Train avg_loss: 6.1918
Validation avg_loss: 1.4089
Saved new best model
Epoch time: 44.08 seconds

Epoch 2/50
Prediction 0 range: 1.8077 to 2.0967
Prediction 1 range: 0.4963 to 3.5079
Prediction 2 range: -1.4010 to 2.2457
Prediction 3 range: 0.0004 to 2.4510
Loss 0: 0.9508
Loss 1: 0.9572
Loss 2: 3.1504
Loss 3: 0.4737
Batch 0, Current loss: 1.3830
Warning: NaN loss at batch 10
Batch 20, Current loss: 2.2029
Batch 30, Current loss: 1.8791
Batch 40, Current loss: 2.1761
Warning: NaN loss at batch 46
Batch 50, Current loss: 1.6086
Batch 60, Current loss: 1.3265
Batch 70, Current loss: 1.1315
Warning: NaN loss at batch 76
Batch 80, Current loss: 1.5308
Warning: NaN loss at batch 82
Warning: NaN loss at batch 83
Batch 90, Current loss: 1.2404
Batch 100, Current loss: 2.4315
Batch 110, Current loss: 2.4215
Warning: NaN loss at batch 115
Batch 120, Current loss: 2.1116
Batch 130, Current loss: 1.1636
Batch 140, Current loss: 1.7180
Batch 150, Current loss: 2.1596
Warning: NaN loss at batch 154
Batch 160, Current loss: 1.6946
Batch 170, Current loss: 1.0097
Train avg_loss: 1.6206
Validation avg_loss: 1.0404
Saved new best model
Epoch time: 43.94 seconds

Epoch 3/50
Prediction 0 range: 1.9926 to 2.8497
Prediction 1 range: -0.8199 to 3.8684
Prediction 2 range: -0.2215 to 3.3631
Prediction 3 range: -0.6443 to 3.0380
Loss 0: 1.2190
Loss 1: 0.5401
Loss 2: 1.4253
Loss 3: 0.3969
Batch 0, Current loss: 0.8953
Batch 10, Current loss: 1.4973
Batch 20, Current loss: 1.7708
Batch 30, Current loss: 1.0362
Warning: NaN loss at batch 36
Batch 40, Current loss: 1.2051
Batch 50, Current loss: 1.5914
Warning: NaN loss at batch 60
Batch 70, Current loss: 1.1756
Batch 80, Current loss: 1.2253
Warning: NaN loss at batch 90
Warning: NaN loss at batch 93
Warning: NaN loss at batch 100
Warning: NaN loss at batch 110
Batch 120, Current loss: 1.2298
Batch 130, Current loss: 1.1173
Warning: NaN loss at batch 137
Batch 140, Current loss: 0.9410
Batch 150, Current loss: 0.9754
Batch 160, Current loss: 2.2945
Batch 170, Current loss: 1.4408
Train avg_loss: 1.2869
Validation avg_loss: 1.9050
Epoch time: 43.35 seconds

Epoch 4/50
Prediction 0 range: 2.0595 to 3.4365
Prediction 1 range: 0.5777 to 4.0582
Prediction 2 range: -2.4243 to 2.0920
Prediction 3 range: 0.7913 to 3.6279
Loss 0: 2.1877
Loss 1: 0.7246
Loss 2: 5.6440
Loss 3: 0.9496
Batch 0, Current loss: 2.3765
Warning: NaN loss at batch 3
Batch 10, Current loss: 0.8486
Warning: NaN loss at batch 16
Batch 20, Current loss: 1.4584
Batch 30, Current loss: 1.4905
Batch 40, Current loss: 1.0014
Batch 50, Current loss: 1.0665
Batch 60, Current loss: 1.5391
Warning: NaN loss at batch 66
Batch 70, Current loss: 0.7611
Batch 80, Current loss: 1.2707
Warning: NaN loss at batch 81
Warning: NaN loss at batch 87
Batch 90, Current loss: 1.3812
Warning: NaN loss at batch 92
Batch 100, Current loss: 0.9739
Batch 110, Current loss: 0.8601
Batch 120, Current loss: 1.1383
Warning: NaN loss at batch 123
Batch 130, Current loss: 0.7440
Batch 140, Current loss: 0.9851
Batch 150, Current loss: 0.7185
Batch 160, Current loss: 1.0907
Batch 170, Current loss: 0.8920
Train avg_loss: 1.0905
Validation avg_loss: 0.9893
Saved new best model
Epoch time: 44.03 seconds

Epoch 5/50
Prediction 0 range: 1.3704 to 2.9697
Prediction 1 range: 1.2740 to 3.6785
Prediction 2 range: 0.8766 to 3.9399
Prediction 3 range: 0.8446 to 3.1579
Loss 0: 1.2949
Loss 1: 0.4331
Loss 2: 0.8844
Loss 3: 0.3353
Batch 0, Current loss: 0.7369
Batch 10, Current loss: 0.6679
Warning: NaN loss at batch 18
Batch 20, Current loss: 1.6071
Batch 30, Current loss: 1.2753
Batch 40, Current loss: 1.2018
Batch 50, Current loss: 1.1449
Batch 60, Current loss: 0.6862
Warning: NaN loss at batch 70
Warning: NaN loss at batch 76
Warning: NaN loss at batch 78
Batch 80, Current loss: 1.1271
Batch 90, Current loss: 0.8513
Batch 100, Current loss: 0.9093
Batch 110, Current loss: 1.2054
Batch 120, Current loss: 1.1227
Batch 130, Current loss: 1.3981
Warning: NaN loss at batch 140
Batch 150, Current loss: 0.6404
Batch 160, Current loss: 1.0636
Warning: NaN loss at batch 162
Warning: NaN loss at batch 168
Batch 170, Current loss: 1.4986
Train avg_loss: 1.0753
Validation avg_loss: 1.1286
Epoch time: 43.33 seconds

Epoch 6/50
Prediction 0 range: 0.9105 to 3.1047
Prediction 1 range: -0.1713 to 4.0944
Prediction 2 range: -0.8242 to 2.0693
Prediction 3 range: -0.1156 to 3.2014
Loss 0: 1.3587
Loss 1: 0.6170
Loss 2: 3.1438
Loss 3: 0.5898
Batch 0, Current loss: 1.4273
Warning: NaN loss at batch 5
Batch 10, Current loss: 0.7406
Warning: NaN loss at batch 11
Batch 20, Current loss: 0.9347
Warning: NaN loss at batch 22
Batch 30, Current loss: 1.0642
Batch 40, Current loss: 0.7860
Batch 50, Current loss: 0.7822
Warning: NaN loss at batch 57
Batch 60, Current loss: 0.9266
Batch 70, Current loss: 0.5777
Warning: NaN loss at batch 71
Batch 80, Current loss: 0.6347
Batch 90, Current loss: 0.7189
Warning: NaN loss at batch 93
Batch 100, Current loss: 0.7632
Warning: NaN loss at batch 105
Batch 110, Current loss: 0.8228
Batch 120, Current loss: 0.6451
Batch 130, Current loss: 0.9278
Batch 140, Current loss: 0.7607
Batch 150, Current loss: 0.9284
Batch 160, Current loss: 0.8124
Batch 170, Current loss: 1.0442
Train avg_loss: 0.9508
Validation avg_loss: 0.9270
Saved new best model
Epoch time: 43.97 seconds

Epoch 7/50
Prediction 0 range: 1.2164 to 3.0987
Prediction 1 range: 1.3717 to 3.8228
Prediction 2 range: 1.2183 to 4.5068
Prediction 3 range: 1.3409 to 3.2507
Loss 0: 0.6572
Loss 1: 0.5031
Loss 2: 1.0662
Loss 3: 0.3312
Batch 0, Current loss: 0.6394
Warning: NaN loss at batch 3
Warning: NaN loss at batch 9
Batch 10, Current loss: 0.6901
Warning: NaN loss at batch 19
Batch 20, Current loss: 1.5795
Warning: NaN loss at batch 29
Batch 30, Current loss: 1.0428
Batch 40, Current loss: 1.0605
Warning: NaN loss at batch 44
Batch 50, Current loss: 1.2368
Batch 60, Current loss: 0.4985
Batch 70, Current loss: 0.7952
Batch 80, Current loss: 0.8016
Batch 90, Current loss: 0.9160
Batch 100, Current loss: 0.6957
Batch 110, Current loss: 0.8232
Batch 120, Current loss: 1.0067
Batch 130, Current loss: 0.9186
Warning: NaN loss at batch 136
Batch 140, Current loss: 0.5798
Batch 150, Current loss: 0.5689
Batch 160, Current loss: 1.1768
Batch 170, Current loss: 0.7142
Warning: NaN loss at batch 172
Train avg_loss: 0.8906
Validation avg_loss: 0.9397
Epoch time: 43.34 seconds

Epoch 8/50
Prediction 0 range: 1.2249 to 3.2823
Prediction 1 range: -0.4526 to 3.9830
Prediction 2 range: 0.7711 to 3.4604
Prediction 3 range: -0.3014 to 3.4774
Loss 0: 1.1943
Loss 1: 0.6354
Loss 2: 1.1248
Loss 3: 0.5593
Batch 0, Current loss: 0.8785
Batch 10, Current loss: 0.6714
Batch 20, Current loss: 0.5119
Batch 30, Current loss: 0.6407
Warning: NaN loss at batch 34
Batch 40, Current loss: 0.8659
Warning: NaN loss at batch 44
Batch 50, Current loss: 0.7594
Batch 60, Current loss: 0.9108
Batch 70, Current loss: 0.8411
Batch 80, Current loss: 0.4832
Warning: NaN loss at batch 84
Batch 90, Current loss: 0.6537
Warning: NaN loss at batch 92
Batch 100, Current loss: 0.5759
Warning: NaN loss at batch 105
Batch 110, Current loss: 1.1250
Batch 120, Current loss: 0.6890
Batch 130, Current loss: 1.4302
Batch 140, Current loss: 0.8278
Batch 150, Current loss: 0.6238
Batch 160, Current loss: 0.9468
Warning: NaN loss at batch 161
Batch 170, Current loss: 0.7843
Warning: NaN loss at batch 177
Train avg_loss: 0.8541
Validation avg_loss: 0.9863
Epoch time: 43.36 seconds

Epoch 9/50
Prediction 0 range: 0.7676 to 3.5927
Prediction 1 range: -0.4823 to 3.6336
Prediction 2 range: -0.6530 to 6.7737
Prediction 3 range: -0.6207 to 3.2750
Loss 0: 1.1531
Loss 1: 0.5563
Loss 2: 1.8312
Loss 3: 0.3568
Batch 0, Current loss: 0.9743
Batch 10, Current loss: 1.2835
Batch 20, Current loss: 0.6326
Warning: NaN loss at batch 24
Batch 30, Current loss: 0.6713
Batch 40, Current loss: 0.5401
Warning: NaN loss at batch 41
Warning: NaN loss at batch 47
Batch 50, Current loss: 1.1560
Warning: NaN loss at batch 53
Batch 60, Current loss: 0.4420
Batch 70, Current loss: 1.2637
Batch 80, Current loss: 1.0278
Batch 90, Current loss: 0.5226
Warning: NaN loss at batch 92
Batch 100, Current loss: 0.7202
Batch 110, Current loss: 1.2181
Batch 120, Current loss: 0.6501
Warning: NaN loss at batch 126
Batch 130, Current loss: 0.6561
Warning: NaN loss at batch 135
Batch 140, Current loss: 1.0415
Batch 150, Current loss: 1.1914
Batch 160, Current loss: 0.9981
Batch 170, Current loss: 0.6214
Train avg_loss: 0.8262
Validation avg_loss: 0.8523
Saved new best model
Epoch time: 43.99 seconds

Epoch 10/50
Prediction 0 range: 1.1957 to 2.6281
Prediction 1 range: 0.9610 to 3.2160
Prediction 2 range: 0.4622 to 3.1921
Prediction 3 range: 1.0889 to 3.3216
Loss 0: 1.4323
Loss 1: 0.6853
Loss 2: 1.4101
Loss 3: 0.6788
Batch 0, Current loss: 1.0516
Batch 10, Current loss: 1.1542
Batch 20, Current loss: 0.8271
Warning: NaN loss at batch 21
Warning: NaN loss at batch 29
Batch 30, Current loss: 0.9488
Batch 40, Current loss: 0.6547
Batch 50, Current loss: 1.1054
Warning: NaN loss at batch 53
Batch 60, Current loss: 0.7651
Batch 70, Current loss: 0.7838
Batch 80, Current loss: 0.9805
Warning: NaN loss at batch 83
Batch 90, Current loss: 0.8600
Warning: NaN loss at batch 91
Batch 100, Current loss: 0.6055
Warning: NaN loss at batch 110
Batch 120, Current loss: 0.6075
Batch 130, Current loss: 0.4117
Batch 140, Current loss: 0.9137
Warning: NaN loss at batch 144
Batch 150, Current loss: 0.8058
Batch 160, Current loss: 1.4438
Batch 170, Current loss: 0.9229
Train avg_loss: 0.7844
Validation avg_loss: 0.9073
Epoch time: 43.38 seconds

Epoch 11/50
Prediction 0 range: 0.7466 to 3.1670
Prediction 1 range: 0.1062 to 3.6593
Prediction 2 range: -0.0881 to 3.0594
Prediction 3 range: -0.3342 to 2.9771
Loss 0: 1.2579
Loss 1: 0.4401
Loss 2: 0.9233
Loss 3: 0.2496
Batch 0, Current loss: 0.7177
Batch 10, Current loss: 0.6871
Batch 20, Current loss: 1.1648
Batch 30, Current loss: 0.5112
Warning: NaN loss at batch 32
Batch 40, Current loss: 0.8455
Batch 50, Current loss: 1.1170
Warning: NaN loss at batch 52
Batch 60, Current loss: 0.6269
Batch 70, Current loss: 0.8534
Batch 80, Current loss: 0.7551
Batch 90, Current loss: 0.9341
Warning: NaN loss at batch 100
Batch 110, Current loss: 0.9340
Warning: NaN loss at batch 119
Batch 120, Current loss: 0.8008
Warning: NaN loss at batch 130
Batch 140, Current loss: 0.8757
Batch 150, Current loss: 0.8653
Warning: NaN loss at batch 151
Batch 160, Current loss: 0.5144
Batch 170, Current loss: 0.7361
Warning: NaN loss at batch 177
Train avg_loss: 0.7764
Validation avg_loss: 0.8952
Epoch time: 43.37 seconds

Epoch 12/50
Prediction 0 range: 1.7223 to 3.2959
Prediction 1 range: -1.4487 to 3.9414
Prediction 2 range: -1.0896 to 2.8471
Prediction 3 range: -0.8706 to 3.3845
Loss 0: 1.5970
Loss 1: 0.4576
Loss 2: 0.9130
Loss 3: 0.2291
Batch 0, Current loss: 0.7992
Batch 10, Current loss: 0.4909
Warning: NaN loss at batch 17
Batch 20, Current loss: 0.7281
Warning: NaN loss at batch 23
Batch 30, Current loss: 0.6895
Batch 40, Current loss: 1.2989
Warning: NaN loss at batch 41
Batch 50, Current loss: 0.6832
Warning: NaN loss at batch 60
Batch 70, Current loss: 0.6012
Batch 80, Current loss: 0.7849
Batch 90, Current loss: 0.9774
Batch 100, Current loss: 0.4906
Warning: NaN loss at batch 104
Batch 110, Current loss: 1.0540
Warning: NaN loss at batch 116
Batch 120, Current loss: 0.7415
Batch 130, Current loss: 0.4139
Batch 140, Current loss: 0.6448
Batch 150, Current loss: 0.6919
Batch 160, Current loss: 0.5262
Batch 170, Current loss: 0.3219
Warning: NaN loss at batch 179
Train avg_loss: 0.7735
Validation avg_loss: 0.9205
Epoch time: 43.37 seconds

Epoch 13/50
Prediction 0 range: 0.9778 to 3.4663
Prediction 1 range: 0.6705 to 4.1868
Prediction 2 range: -0.3341 to 2.9529
Prediction 3 range: -0.0673 to 3.8181
Loss 0: 1.3745
Loss 1: 0.7812
Loss 2: 0.9614
Loss 3: 0.5500
Batch 0, Current loss: 0.9168
Batch 10, Current loss: 0.8354
Batch 20, Current loss: 0.5700
Batch 30, Current loss: 0.5154
Warning: NaN loss at batch 31
Warning: NaN loss at batch 36
Batch 40, Current loss: 0.9029
Warning: NaN loss at batch 41
Batch 50, Current loss: 0.6820
Batch 60, Current loss: 0.7720
Warning: NaN loss at batch 70
Batch 80, Current loss: 0.7245
Batch 90, Current loss: 0.6803
Batch 100, Current loss: 0.7437
Batch 110, Current loss: 0.5790
Batch 120, Current loss: 0.9875
Warning: NaN loss at batch 128
Batch 130, Current loss: 1.0902
Batch 140, Current loss: 0.7028
Batch 150, Current loss: 0.5379
Batch 160, Current loss: 0.7083
Batch 170, Current loss: 0.5447
Warning: NaN loss at batch 176
Train avg_loss: 0.7443
Validation avg_loss: 0.8891
Epoch time: 43.51 seconds

Epoch 14/50
Prediction 0 range: 1.3988 to 3.3519
Prediction 1 range: -0.0496 to 3.1291
Prediction 2 range: 0.6460 to 4.1839
Prediction 3 range: 0.2643 to 3.3542
Loss 0: 0.7594
Loss 1: 0.5375
Loss 2: 0.7025
Loss 3: 0.1747
Batch 0, Current loss: 0.5435
Batch 10, Current loss: 0.4379
Batch 20, Current loss: 0.6699
Batch 30, Current loss: 0.4834
Batch 40, Current loss: 0.5451
Warning: NaN loss at batch 48
Batch 50, Current loss: 0.5304
Batch 60, Current loss: 0.5895
Batch 70, Current loss: 0.5627
Batch 80, Current loss: 0.6598
Warning: NaN loss at batch 85
Batch 90, Current loss: 0.7587
Batch 100, Current loss: 0.4639
Warning: NaN loss at batch 110
Warning: NaN loss at batch 117
Batch 120, Current loss: 0.8287
Warning: NaN loss at batch 122
Batch 130, Current loss: 1.0993
Warning: NaN loss at batch 133
Batch 140, Current loss: 1.2070
Batch 150, Current loss: 1.0009
Batch 160, Current loss: 0.4781
Batch 170, Current loss: 0.7691
Warning: NaN loss at batch 177
Train avg_loss: 0.7098
Validation avg_loss: 0.9274
Epoch time: 43.36 seconds

Epoch 15/50
Prediction 0 range: 1.0250 to 3.5327
Prediction 1 range: 0.9869 to 4.3486
Prediction 2 range: 1.2075 to 4.8333
Prediction 3 range: 0.9992 to 4.0440
Loss 0: 0.9944
Loss 1: 0.3149
Loss 2: 0.8331
Loss 3: 0.2896
Batch 0, Current loss: 0.6080
Batch 10, Current loss: 0.5078
Batch 20, Current loss: 0.4145
Warning: NaN loss at batch 23
Batch 30, Current loss: 0.4219
Batch 40, Current loss: 0.7472
Warning: NaN loss at batch 45
Batch 50, Current loss: 0.7158
Warning: NaN loss at batch 58
Warning: NaN loss at batch 60
Batch 70, Current loss: 0.8565
Batch 80, Current loss: 0.6617
Batch 90, Current loss: 1.7108
Warning: NaN loss at batch 91
Batch 100, Current loss: 0.6176
Batch 110, Current loss: 0.9500
Batch 120, Current loss: 0.9135
Batch 130, Current loss: 0.6071
Batch 140, Current loss: 0.4644
Batch 150, Current loss: 1.0265
Warning: NaN loss at batch 158
Batch 160, Current loss: 0.8051
Warning: NaN loss at batch 168
Batch 170, Current loss: 0.7814
Train avg_loss: 0.7047
Validation avg_loss: 0.9203
Epoch 00015: reducing learning rate of group 0 to 5.0000e-05.
Epoch time: 43.36 secondsQXcbConnection: Failed to initialize XRandr
Qt: XKEYBOARD extension not present on the X server.

Bad key "text.kerning_factor" on line 4 in
/home/nudt_cleng/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.
You probably need to get an updated matplotlibrc file from
https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template
or from the matplotlib source distribution


Epoch 16/50
Prediction 0 range: 1.4299 to 3.8461
Prediction 1 range: 1.0913 to 3.9612
Prediction 2 range: 0.1800 to 4.0091
Prediction 3 range: 1.3681 to 3.8736
Loss 0: 0.8180
Loss 1: 0.2168
Loss 2: 0.6304
Loss 3: 0.2648
Batch 0, Current loss: 0.4825
Warning: NaN loss at batch 8
Batch 10, Current loss: 0.6172
Warning: NaN loss at batch 15
Batch 20, Current loss: 0.4229
Warning: NaN loss at batch 21
Batch 30, Current loss: 0.4662
Batch 40, Current loss: 0.4349
Batch 50, Current loss: 0.9503
Batch 60, Current loss: 0.5151
Batch 70, Current loss: 0.7404
Warning: NaN loss at batch 73
Warning: NaN loss at batch 80
Warning: NaN loss at batch 85
Batch 90, Current loss: 0.5889
Batch 100, Current loss: 0.5931
Batch 110, Current loss: 0.5166
Warning: NaN loss at batch 113
Batch 120, Current loss: 0.9163
Batch 130, Current loss: 0.3989
Batch 140, Current loss: 0.4026
Batch 150, Current loss: 1.0650
Batch 160, Current loss: 0.4419
Batch 170, Current loss: 0.8564
Train avg_loss: 0.6352
Validation avg_loss: 0.9260
Epoch time: 43.37 seconds

Epoch 17/50
Prediction 0 range: 1.1259 to 2.9483
Prediction 1 range: -1.3496 to 4.1343
Prediction 2 range: 1.1349 to 3.4910
Prediction 3 range: -0.8330 to 3.6045
Loss 0: 1.4947
Loss 1: 0.2840
Loss 2: 0.9669
Loss 3: 0.1686
Batch 0, Current loss: 0.7286
Batch 10, Current loss: 0.9381
Batch 20, Current loss: 0.4618
Warning: NaN loss at batch 22
Batch 30, Current loss: 0.7517
Warning: NaN loss at batch 35
Batch 40, Current loss: 0.6760
Warning: NaN loss at batch 46
Batch 50, Current loss: 0.5199
Batch 60, Current loss: 0.6460
Batch 70, Current loss: 0.6593
Batch 80, Current loss: 0.7615
Warning: NaN loss at batch 85
Batch 90, Current loss: 0.4830
Batch 100, Current loss: 0.4541
Batch 110, Current loss: 0.5785
Batch 120, Current loss: 0.7271
Warning: NaN loss at batch 126
Batch 130, Current loss: 0.5043
Batch 140, Current loss: 0.5805
Batch 150, Current loss: 0.4240
Batch 160, Current loss: 0.5708
Warning: NaN loss at batch 162
Warning: NaN loss at batch 165
Batch 170, Current loss: 0.5903
Train avg_loss: 0.6279
Validation avg_loss: 0.8612
Epoch time: 43.35 seconds

Epoch 18/50
Prediction 0 range: 1.1590 to 3.6704
Prediction 1 range: 0.4682 to 4.0032
Prediction 2 range: 0.2780 to 4.7053
Prediction 3 range: 0.5813 to 3.8280
Loss 0: 0.8355
Loss 1: 0.4101
Loss 2: 0.7344
Loss 3: 0.2639
Batch 0, Current loss: 0.5610
Batch 10, Current loss: 0.4275
Batch 20, Current loss: 0.5374
Batch 30, Current loss: 0.6230
Batch 40, Current loss: 0.6556
Batch 50, Current loss: 0.5910
Warning: NaN loss at batch 59
Batch 60, Current loss: 0.5781
Warning: NaN loss at batch 69
Batch 70, Current loss: 0.7997
Warning: NaN loss at batch 73
Batch 80, Current loss: 0.7275
Warning: NaN loss at batch 86
Batch 90, Current loss: 0.7769
Batch 100, Current loss: 0.7001
Warning: NaN loss at batch 101
Batch 110, Current loss: 0.6248
Batch 120, Current loss: 0.5209
Warning: NaN loss at batch 125
Batch 130, Current loss: 0.7584
Warning: NaN loss at batch 139
Batch 140, Current loss: 0.7441
Batch 150, Current loss: 0.6081
Batch 160, Current loss: 0.5866
Batch 170, Current loss: 0.6252
Train avg_loss: 0.6237
Validation avg_loss: 0.8666
Epoch time: 43.36 seconds

Epoch 19/50
Prediction 0 range: 1.2795 to 3.5162
Prediction 1 range: 0.0587 to 3.1523
Prediction 2 range: 1.1822 to 3.4807
Prediction 3 range: -0.0977 to 3.7148
Loss 0: 1.3592
Loss 1: 0.1703
Loss 2: 0.9399
Loss 3: 0.1037
Batch 0, Current loss: 0.6433
Batch 10, Current loss: 0.6000
Warning: NaN loss at batch 15
Batch 20, Current loss: 0.4110
Batch 30, Current loss: 0.3757
Batch 40, Current loss: 0.4004
Batch 50, Current loss: 0.5187
Batch 60, Current loss: 0.5770
Warning: NaN loss at batch 67
Batch 70, Current loss: 0.6630
Batch 80, Current loss: 0.9186
Warning: NaN loss at batch 81
Batch 90, Current loss: 0.3393
Batch 100, Current loss: 0.8342
Batch 110, Current loss: 0.9888
Warning: NaN loss at batch 118
Warning: NaN loss at batch 119
Batch 120, Current loss: 0.7650
Warning: NaN loss at batch 121
Batch 130, Current loss: 0.4395
Warning: NaN loss at batch 137
Batch 140, Current loss: 0.3099
Batch 150, Current loss: 0.8378
Batch 160, Current loss: 0.8266
Batch 170, Current loss: 0.4490
Train avg_loss: 0.6219
Validation avg_loss: 0.8526
Early stopping triggered

Total training time: 827.14 seconds

Loading best model for testing...

Starting model testing...
Processed test batch: 0
Processed test batch: 10
Processed test batch: 20
Processed test batch: 30

Test average loss: nan

Test prediction statistics:

Output 1:
Mean: 2.1172
Std: 0.4950
Min: 0.0000
Max: 4.0119

Output 2:
Mean: 2.1954
Std: 0.7769
Min: -0.9111
Max: 4.7201

Output 3:
Mean: 2.1008
Std: 0.8435
Min: -3.4188
Max: 4.2758

Output 4:
Mean: 2.2271
Std: 0.8912
Min: -0.9067
Max: 3.7421

Training ensemble models...

Lasso ensemble results:
Weights: [0.         0.41107273 0.47383685 0.10010625]
rmse: 0.8338
mae: 0.6538
r2: 0.5031
pearson: 0.7189

Elastic ensemble results:
Weights: [0.         0.17244018 0.26918713 0.14908191]
rmse: 0.9249
mae: 0.7322
r2: 0.3886
pearson: 0.7151

Rf ensemble results:
Weights: [0.12505101 0.22860245 0.50309587 0.14325066]
rmse: 0.3197
mae: 0.2444
r2: 0.9269
pearson: 0.9686

Gradientboost ensemble results:
Weights: [0.09130235 0.24387931 0.56199183 0.1028265 ]
rmse: 0.5547
mae: 0.4442
r2: 0.7801
pearson: 0.8874

Training and evaluation completed successfully!
